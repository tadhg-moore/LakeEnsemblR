#' Run Ensemble of lake models
#'
#' Run each of the lake models
#'
#' @param config_file filepath; to LakeEnsemblr yaml master config file
#' @param model vector; model to export driving data. Options include
#'   c('GOTM', 'GLM', 'Simstrat', 'FLake', 'MyLake')
#' @param folder filepath; to folder which contains the model folders generated by export_config()
#' @param return_list boolean; Return a list of dataframes of model output. Defaults to FALSE
#' @param create_netcdf boolean; Create ensemble output file. Defaults to TRUE
#' @import ncdf4
#' @importFrom FLakeR run_flake
#' @importFrom GLM3r run_glm
#' @importFrom GOTMr run_gotm
#' @importFrom SimstratR run_simstrat
#' @importFrom MyLakeR run_mylake
#' @importFrom gotmtools get_yaml_value get_vari
#' @importFrom rLakeAnalyzer get.offsets
#' @importFrom reshape2 dcast
#' @importFrom glmtools get_nml_value get_var
#' @importFrom lubridate year round_date seconds_to_period
#'
#' @export
run_ensemble <- function(config_file, model = c("GOTM", "GLM", "Simstrat", "FLake", "MyLake"),
                         folder = ".", return_list = FALSE, create_netcdf = TRUE) {

  if(any(duplicated(model))){
    stop('model input argument cannot contain duplicates')
  }
  # It's advisable to set timezone to GMT in order to avoid errors when reading time
  original_tz  <-  Sys.getenv("TZ")
  Sys.setenv(TZ = "GMT")
  tz  <-  "UTC"

  # Set working directory
  oldwd <- getwd()

  # this way if the function exits for any reason, success or failure, these are reset:
  on.exit({
    setwd(oldwd)
    Sys.setenv(TZ = original_tz)
  })

  ## Extract start, stop, lat & lon for netCDF file from config file
  start <- get_yaml_value(config_file, "time", "start")
  stop <- get_yaml_value(config_file, "time", "stop")
  lat <- get_yaml_value(config_file, "location", "latitude")
  lon <- get_yaml_value(config_file, "location", "longitude")
  obs_file <- get_yaml_value(config_file, "observations", "file")

  # Get output configurations
  out_file <- get_yaml_value(config_file, "output", "file")
  out_depths <- get_yaml_value(config_file, "output", "depths")
  format <- get_yaml_value(config_file, "output", "format")
  time_unit <- get_yaml_value(config_file, "output", "time_unit")
  time_step <- get_yaml_value(config_file, "output", "time_step")
  out_vars <- get_yaml_value(config_file, "output", "variables")

  if(create_netcdf){
    compression <- get_yaml_value(config_file, "output", "compression")
  }


  # Create output time vector
  out_time <- data.frame(datetime = seq.POSIXt(as.POSIXct(start, tz = tz), as.POSIXct(stop, tz = tz),
                         by = paste(time_step, time_unit)))


  if(obs_file != "NULL"){
    message("Loading obs_file...")
    obs <- read.csv(obs_file, stringsAsFactors = FALSE)
    obs_deps <- unique(obs$Depth_meter)

    # change data format from long to wide
    obs_out <- reshape2::dcast(obs, datetime ~ Depth_meter, value.var = "Water_Temperature_celsius")
    str_depths <- colnames(obs_out)[2:ncol(obs_out)]
    colnames(obs_out) <- c("datetime", paste("wtr_", str_depths, sep = ""))
    obs_out$datetime <- as.POSIXct(obs_out$datetime, tz = tz)

    # Subset to out_time
    obs_out <- obs_out[obs_out$datetime %in% out_time$datetime, ]
    obs_out <- merge(out_time, obs_out, by = "datetime", all.x = TRUE)

  }else{
    obs_deps <- NULL
  }

  run_model_args <- list(config_file = config_file,
                         folder = folder,
                         return_list = return_list,
                         create_netcdf = create_netcdf,
                         tz = tz,
                         start = start,
                         stop = stop,
                         obs_deps = obs_deps,
                         out_time = out_time,
                         out_vars = out_vars,
                         time_step = time_step)

  model_out <- setNames(
    lapply(model, function(mod_name) do.call(paste0('.run_', mod_name),
                                             run_model_args)),
    model
  )


  if(return_list | create_netcdf){

    if("temp" %in% out_vars){
      temp_list <- setNames(
        lapply(model, function(mod_name) model_out[[mod_name]][['temp']]),
        paste0(model,'_watertemp')
      )
      if(!is.null(obs_deps)){
        temp_list <- append(temp_list, list("Obs_watertemp" = obs_out))
      }

    }

    if("ice_height" %in% out_vars){
      ice_list <- setNames(
        lapply(model, function(mod_name) model_out[[mod_name]][['ice_height']]),
        paste0(model,'_ice_height')
      )
    }

    if(create_netcdf){

      if("temp" %in% out_vars){
        lengths <- lapply(temp_list, ncol) # Extract ncols in each output
        lon_list <- which.max(lengths) # Select largest depths
        deps <- get.offsets(temp_list[[lon_list]]) # Extract depths
        deps <- deps
      }

      # Creat output directory
      message("Creating directory for output: ", file.path(folder, "output"))
      dir.create(file.path(folder, "output"), showWarnings = FALSE)

      #Create ncdf
      message("Creating NetCDF file [", Sys.time(), "]")
      ref_time <- as.POSIXct("1970-01-01 00:00:00", tz = "GMT") # Reference time for netCDF time
      # Calculate seconds since reference time
      nsecs <- as.numeric(difftime(out_time$datetime, ref_time, units = "secs"))
      xvals <- 180 - lon # Convert longitude to degrees east
      yvals <- lat # Latitude
      # Define lon and lat dimensions
      lon1 <- ncdim_def("lon", "degrees_east", vals = as.double(xvals))
      lat2 <- ncdim_def("lat", "degrees_north", vals = as.double(yvals))

      # Set dimensions
      # Time dimension
      timedim <- ncdim_def("time", units = "seconds since 1970-01-01 00:00:00",
                           vals = as.double(nsecs), calendar = "proleptic_gregorian")
      if("temp" %in% out_vars){
        # Depth dimension
        depthdim <- ncdim_def("z", units = "meters", vals = as.double(rev(deps)),
                              longname = "Depth from surface")
      }

      fillvalue <- 1e20 # Fill value
      missvalue <- 1e20 # Missing value

      nc_vars <- list() #Initialize empty list to fill netcdf variables

      if("temp" %in% out_vars){
        for(i in seq_len(length(temp_list))) {

          model_name <- strsplit(names(temp_list)[i], "_")[[1]][1]

          lname <- paste(model_name, "Water temperature") # Long name
          # Define variable
          tmp_def <- ncvar_def(paste0(tolower(names(temp_list)[i])), "Celsius",
                               list(lon1, lat2, timedim, depthdim), fillvalue, lname,
                               prec = "float", compression = compression, shuffle = FALSE)
          nc_vars[[length(nc_vars) + 1]] <- tmp_def # Add to list
        }
        names(nc_vars)[(length(nc_vars) - length(temp_list) + 1):length(nc_vars)] <-
          paste0(names(temp_list), "_temp") # Re-assign list names
      }

      if("ice_height" %in% out_vars){
        for(i in seq_len(length(ice_list))) {
          model_name <- strsplit(names(ice_list)[i], "_")[[1]][1]
          # Long name
          lname <- paste(model_name, "Ice height")
          # Define variable
          tmp_def <- ncvar_def(paste0(tolower(names(ice_list)[i])), "m", list(lon1, lat2, timedim),
                               fillvalue, lname, prec = "float", compression = compression,
                               shuffle = FALSE)
          nc_vars[[length(nc_vars) + 1]] <- tmp_def # Add to list
        }
        # Re-assign list names
        names(nc_vars)[(length(nc_vars) - length(ice_list) + 1):length(nc_vars)] <-
          paste0(names(ice_list), "_ice_height")
      }

      # Create file name for output file
      fname <- file.path(folder, "output", out_file) # Ensemble output filename

      # If file exists - delete it
      if(file.exists(fname)){
        unlink(fname, recursive = TRUE)
      }

      # Create and input data into the netCDF file
      ncout <- nc_create(fname, nc_vars, force_v4 = T)
      # Add coordinates attribute for use with get_vari()
      ncatt_put(ncout, "z", attname = "coordinates", attval = c("z"))


      # Loop through and add each variable

      # Add tryCatch ensure that it closes netCDF file
      result <- tryCatch({

        if("temp" %in% out_vars){
          for(i in seq_len(length(temp_list))) {
            mat1 <- matrix(NA, nrow = nc_vars[[i]]$dim[[3]]$len, ncol = nc_vars[[i]]$dim[[4]]$len)

            deps_temp <- get.offsets(temp_list[[i]]) # vector of depths to input into the matrix
            mat <- as.matrix(temp_list[[i]][, -1])

            for(j in seq_len(ncol(mat))) {
              col <- which(deps == deps_temp[j])
              mat1[, col] <- mat[, j]
            }
            # mat1[1:nrow(mat),1:ncol(mat)] <- mat

            ncvar_put(ncout, nc_vars[[i]], mat1)
            ncatt_put(ncout, nc_vars[[i]], attname = "coordinates", attval = c("lon lat z"))
            ncvar_change_missval(ncout, nc_vars[[i]], missval = fillvalue)
          }
        }

        if("ice_height" %in% out_vars){
          for(i in seq_len(length(ice_list))) {

            mat <- as.matrix(ice_list[[i]][, -1])

            #Create index for nc_vars
            nc_idx <- length(nc_vars) - length(ice_list) + i

            ncvar_put(ncout, nc_vars[[nc_idx]], mat)
            ncatt_put(ncout, nc_vars[[nc_idx]], attname = "coordinates", attval = c("lon lat"))
            ncvar_change_missval(ncout, nc_vars[[i]], missval = fillvalue)
          }
        }

      }, warning = function(w) {
        return_val <- "Warning"
      }, error = function(e) {
        return_val <- "Error"
        warning("Error creating netCDF file!")
      }, finally = {
        nc_close(ncout) # Close netCDF file
      })

      message("Finished writing NetCDF file [", Sys.time(), "]")

    }


  }

  # Set the timezone back to the original
  Sys.setenv(TZ = original_tz)

  if(return_list){
    return(temp_list)
  }
}


#' @keywords internal
.run_GLM <- function(config_file, folder, return_list, create_netcdf, tz, start, stop, obs_deps, out_time, out_hour, out_vars, time_step){
  #Need to input start and stop into nml file
  nml_file <- file.path(folder, get_yaml_value(config_file, "config_files", "glm"))
  input_nml(nml_file, label = "time", key = "start", value = paste0("'", start, "'"))
  input_nml(nml_file, label = "time", key = "stop", value = paste0("'", stop, "'"))

  # Update model output
  input_nml(nml_file, label = "output", key = "nsave", value = time_step)

  #Delete previous output
  # out_folder <- get_json_value(file = file.path(folder, par_fpath), label = 'Output', 'Path')
  old_output <- list.files(file.path(folder, "GLM", "output"))
  unlink(file.path(folder, "GLM", "output", old_output), recursive = TRUE)

  run_glm(sim_folder = file.path(folder, "GLM"))

  message("GLM run is complete! ", paste0("[", Sys.time(), "]"))

  if(return_list | create_netcdf) {

    # Extract output
    glm_out <- get_output(config_file = config_file, model = "GLM", vars = out_vars,
                          obs_depths = obs_deps, folder = folder)

    # Ensure GLM is on the same time step for output
    if(!is.list(glm_out)) {
      glm_out <- merge(glm_out, out_time, by = "datetime", all.y = TRUE)
    } else {
      glm_out <- lapply(seq_len(length(glm_out)), function(x){
        merge(glm_out[[x]], out_time, by = 1, all.y = TRUE)
      })
      names(glm_out) <- out_vars # Re-assign names to list
    }

  }
  return(glm_out)
}

#' @keywords internal
.run_FLake <- function(config_file, folder, return_list, create_netcdf, tz, start, stop, obs_deps, out_time, out_hour, out_vars, time_step){
  #Need to figure out how to subset data by dates
  nml_file <- get_yaml_value(config_file, "config_files", "flake")
  nml_file <- file.path(folder, nml_file)
  if(file.exists(file.path(folder, "FLake", "all_meteo_file.dat"))){
    met_file <- file.path(folder, "FLake", "all_meteo_file.dat")
    met_outfile <- file.path(folder, "FLake", "meteo_file_tmp.dat")
  }else{
    met_file <- suppressWarnings(get_nml_value(arg_name = "meteofile", nml_file = nml_file))
    met_file <- gsub(",", "", met_file)
    met_file <- file.path(folder, "FLake", met_file)
    met_outfile <- file.path(folder, "FLake", met_file)
  }
  message("FLake: Loading ", met_file)
  met <- read.delim(met_file, header = FALSE, stringsAsFactors = FALSE)
  colnames(met)[ncol(met)] <- "datetime"
  met$datetime <- as.POSIXct(met$datetime, tz = tz)
  met_sub <- met[(met$datetime >= start & met$datetime <= stop), ]
  if(nrow(met_sub) < nrow(met)){
    warning("FLake: Writing new met file with shorter time series: ", met_outfile)
  }

  # Write to file
  write.table(met_sub, met_outfile, sep = "\t", quote = FALSE, col.names = FALSE,
              row.names = FALSE)
  met_outfile <- basename(met_outfile)
  input_nml(nml_file, "SIMULATION_PARAMS", "time_step_number", nrow(met_sub))
  input_nml(nml_file, "METEO", "meteofile", paste0("'", met_outfile, "'"))

  # Select nml file again
  nml_file <- basename(get_yaml_value(config_file, "config_files", "flake"))

  #Delete previous output
  old_output <- list.files(file.path(folder, "FLake", "output"))
  unlink(file.path(folder, "FLake", "output", old_output), recursive = TRUE)


  run_flake(sim_folder = file.path(folder, "FLake"), nml_file = nml_file)

  if(return_list | create_netcdf){

    met_timestep <- get_yaml_value(config_file, "meteo", "time_step")
    out_hour <- ifelse(met_timestep == 86400, hour(start), 0) #Used for FLake output

    # Extract output
    fla_out <- get_output(config_file = config_file, model = "FLake", vars = out_vars,
                          obs_depths = obs_deps, folder = folder, out_time = out_time,
                          out_hour = out_hour)

    # Ensure FLake is on the same time step for output
    if(!is.list(fla_out)) {
      fla_out <- merge(fla_out, out_time, by = "datetime", all.y = TRUE)
    } else {
      fla_out <- lapply(seq_len(length(fla_out)), function(x){
        merge(fla_out[[x]], out_time, by = 1, all.y = TRUE)
      })
      names(fla_out) <- out_vars # Re-assign names to list
    }
  }
  message("FLake run is complete! ", paste0("[", Sys.time(), "]"))
  return(fla_out)

}

#' @keywords internal
.run_GOTM <- function(config_file, folder, return_list, create_netcdf, tz, start, stop, obs_deps, out_time, out_vars, time_step){

  # Need to input start and stop into yaml file
  time_method <- get_yaml_value(config_file, "output", "time_method")
  time_unit <- get_yaml_value(config_file, "output", "time_unit")

  yaml_file <- file.path(folder, get_yaml_value(config_file, "config_files", "gotm"))
  input_yaml(yaml_file, label = "time", key = "start", value = start)
  input_yaml(yaml_file, label = "time", key = "stop", value = stop)

  #output yaml
  out_yaml <- file.path(folder, "GOTM", "output.yaml")
  input_yaml(out_yaml, label = "output", key = "time_method", value = time_method)
  input_yaml(out_yaml, label = "output", key = "time_unit", value = time_unit)
  input_yaml(out_yaml, label = "output", key = "time_step", value = time_step)
  input_yaml(out_yaml, label = "output", key = "format", value = "netcdf")

  #Delete previous output
  old_output <- list.files(file.path(folder, "GOTM", "output"))
  unlink(file.path(folder, "GOTM", "output", old_output), recursive = TRUE)

  run_gotm(sim_folder = file.path(folder, "GOTM"), yaml_file = basename(yaml_file))

  message("GOTM run is complete! ", paste0("[", Sys.time(), "]"))

  if(return_list | create_netcdf){

    # Extract output
    gotm_out <- get_output(config_file = config_file, model = "GOTM", vars = out_vars,
                           obs_depths = obs_deps, folder = folder)

    # Ensure GOTM is on the same time step for output
    if(!is.list(gotm_out)) {
      gotm_out <- merge(gotm_out, out_time, by = "datetime", all.y = T)
    } else {
      gotm_out <- lapply(seq_len(length(gotm_out)), function(x){
        merge(gotm_out[[x]], out_time, by = 1, all.y = T)
      })
      names(gotm_out) <- out_vars # Re-assign names to list
    }
  }

  return(gotm_out)
}

#' @keywords internal
.run_Simstrat <- function(config_file, folder, return_list, create_netcdf, tz, start, stop, obs_deps, out_time, out_vars, time_step){
  sim_par <- get_yaml_value(config_file, "config_files", "simstrat")

  # Set times
  reference_year <- year(as.POSIXct(start))
  input_json(sim_par, "Simulation", "Start year", reference_year)
  start_date_simulation <- as.POSIXct(start, format = "%Y-%m-%d %H:%M:%S", tz = tz)
  end_date_simulation <- as.POSIXct(stop, format = "%Y-%m-%d %H:%M:%S", tz = tz)
  input_json(sim_par, "Simulation", "Start d",
             as.numeric(difftime(start_date_simulation,
                                 as.POSIXct(paste0(reference_year, "-01-01")), units = "days")))
  input_json(sim_par, "Simulation", "End d",
             as.numeric(difftime(end_date_simulation,
                                 as.POSIXct(paste0(reference_year, "-01-01")), units = "days")))
  input_json(sim_par, "Output", "Times", time_step)

  # Need to input start and stop into json par file
  par_fpath <- get_yaml_value(config_file, "config_files", "simstrat")
  par_file <- basename(par_fpath)

  #Delete previous output
  # out_folder <- get_json_value(file = file.path(folder, par_fpath), label = "Output", "Path")
  old_output <- list.files(file.path(folder, "Simstrat", "output"))
  unlink(file.path(folder, "Simstrat", "output", old_output), recursive = TRUE)


  run_simstrat(sim_folder = file.path(folder, "Simstrat"), par_file = par_file, verbose = FALSE)

  message("Simstrat run is complete! ", paste0("[", Sys.time(), "]"))

  if(return_list | create_netcdf){

    ### Extract output
    sim_out <- get_output(config_file = config_file, model = "Simstrat", vars = out_vars,
                          obs_depths = obs_deps, folder = folder)

    # Ensure Simstrat is on the same time step for output
    if(!is.list(sim_out)) {
      sim_out <- merge(sim_out, out_time, by = "datetime", all.y = T)
    } else {
      sim_out <- lapply(seq_len(length(sim_out)), function(x){
        merge(sim_out[[x]], out_time, by = 1, all.y = T)
      })
      names(sim_out) <- out_vars # Re-assign names to list
    }
  }
  return(sim_out)
}

#' @keywords internal
.run_MyLake <- function(config_file, folder, return_list, create_netcdf, tz, start, stop, obs_deps, out_time, out_vars, time_step){
  run_mylake(sim_folder = folder)

  message("MyLake run is complete! ", paste0("[", Sys.time(), "]"))

  if(return_list | create_netcdf){

    ### Extract output
    mylake_out <- get_output(config_file = config_file, model = "MyLake", vars = out_vars,
                             obs_depths = obs_deps, folder = folder)

    if(!is.list(mylake_out)) {
      mylake_out <- merge(mylake_out, out_time, by = "datetime", all.y = T)
    } else {
      mylake_out <- lapply(seq_len(length(mylake_out)), function(x){
        merge(mylake_out[[x]], out_time, by = 1, all.y = T)
      })
      names(mylake_out) <- out_vars # Re-assign names to list
    }
  }
  return(mylake_out)
}